{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff01bc1e-eabd-4f79-bac2-cff687ee275e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Retail Catalog Upsert with PySpark and Delta Lake\n",
    "\n",
    "A retail company receives daily updates for its product catalog, including new products, price changes, and discontinued items.  \n",
    "Instead of overwriting the entire catalog or simply appending new records, the goal is to **upsert** the incoming data—updating existing products with the latest information and inserting new products—so that the product catalog remains **accurate, consistent, and up-to-date in real-time**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca0145a6-b941-4698-98fb-3b8f1b8d711a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SQL: Table Setup\n",
    "\n",
    "### Explanation:\n",
    "- Drops the table learning_db.qna.products if it exists, ensuring a clean slate.\n",
    "- Creates the table with columns for product details.\n",
    "- Inserts three initial product records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a357af9-388b-42b4-94b8-3e704c8e4229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS learning_db.qna.products;\n",
    "CREATE TABLE IF NOT EXISTS learning_db.qna.products (\n",
    "  product_id STRING,\n",
    "  name STRING,\n",
    "  category STRING,\n",
    "  price DOUBLE,\n",
    "  currency STRING,\n",
    "  updated_at TIMESTAMP);\n",
    "\n",
    "INSERT INTO learning_db.qna.products VALUES\n",
    "('1', 'Wireless Mouse', 'Electronics', 25.99, 'USD', '2025-08-20 10:00:00'),\n",
    "('2', 'Yoga Mat', 'Fitness', 19.99, 'USD', '2025-08-20 10:05:00'),\n",
    "('3', 'Coffee Mug', 'Kitchen', 7.50, 'USD', '2025-08-20 10:10:00');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8e9bda5-afb6-42e0-a582-92c048c79160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Python: Load Table as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67cf0a33-0ea9-4d3f-8714-f958843e59d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"learning_db.qna.products\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f438083b-6488-4d08-8da2-26e87c6e0191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Python: Delta Lake Merge\n",
    "Explanation:\n",
    "\n",
    "- Imports Delta Lake functionality.\n",
    "- Checks if the Delta table path /Volumes/learning_db/qna/landing/produncts_upsert exists.\n",
    "- If it exists:\n",
    "  - Loads the Delta table.\n",
    "  - Merges the new data (`df`) into the Delta table:\n",
    "  - Updates records if the incoming `updated_at` is newer or equal.\n",
    "  - Inserts records if they don’t exist.\n",
    "- If it does not exist:\n",
    "  - Writes the DataFrame as a new Delta table at the specified path.\n",
    "- Displays the merged Delta table, ordered by `product_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cebe2fcb-368b-4ae6-99fb-4d6d1a16b0dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating Delta Object\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "if dbutils.fs.ls(\"/Volumes/learning_db/qna/landing/produncts_upsert\"):\n",
    "    \n",
    "    deltaObject = DeltaTable.forPath(spark, \"/Volumes/learning_db/qna/landing/produncts_upsert\")\n",
    "\n",
    "    deltaObject.alias(\"tgt\").merge(\n",
    "        df.alias(\"src\"),\n",
    "        \"src.product_id = tgt.product_id\",\n",
    "    ).whenMatchedUpdateAll(condition=\"src.updated_at >= tgt.updated_at\")\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "\n",
    "else:\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/learning_db/qna/landing/produncts_upsert\")\n",
    "\n",
    "display(spark.sql(\"select * from delta.`/Volumes/learning_db/qna/landing/produncts_upsert`\").orderBy(\"product_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94bf28a-3e7a-43be-9fdb-eb54c6f2b281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SQL: Insert More Products\n",
    "### Explanation:\n",
    "- Inserts five new records into the products table.\n",
    "  - Some have duplicate product_id values (for updates).\n",
    "  - Some are new products.\n",
    "  - One is a duplicate row (same as previous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1045804f-b1fc-4a27-a5e7-3b267546186b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "INSERT INTO learning_db.qna.products VALUES\n",
    "('2', 'Yoga Mat Pro', 'Fitness', 24.99, 'USD', '2025-08-20 12:00:00'), -- duplicate id, updated info\n",
    "('3', 'Coffee Mug XL', 'Kitchen', 9.99, 'USD', '2025-08-20 12:05:00'), -- duplicate id, updated info\n",
    "('4', 'Bluetooth Speaker', 'Electronics', 49.99, 'USD', '2025-08-20 12:10:00'), -- new\n",
    "('5', 'Running Shoes', 'Fitness', 59.99, 'USD', '2025-08-20 12:15:00'), -- new\n",
    "('5', 'Running Shoes', 'Fitness', 59.99, 'USD', '2025-08-20 12:15:00'); -- same as above, duplicate row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57fd9472-a5ed-479f-b463-44bc4c11f5ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Python: Deduplicate and Keep Latest\n",
    "### Explanation:\n",
    "\n",
    "- Imports necessary PySpark functions and windowing.\n",
    "- Reloads the `products` table into a DataFrame.\n",
    "- Defines a window partitioned by `product_id`, ordered by `updated_at` descending.\n",
    "- Assigns a `row number` within each partition (latest record gets `rnk=1`).\n",
    "- Filters to keep only the latest record for each `product_id` (deduplication).\n",
    "- Displays the deduplicated DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a669a2c6-51f6-493d-b0dd-38d055581892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = spark.table(\"learning_db.qna.products\")\n",
    "\n",
    "WindowSpec = Window.partitionBy(\"product_id\").orderBy(desc(\"updated_at\"))\n",
    "\n",
    "df_rnk = df.withColumn(\"rnk\", row_number().over(WindowSpec))\n",
    "df = df_rnk.filter(col(\"rnk\") == 1).drop(\"rnk\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c299cd-f990-4c87-add3-3212f5f3a583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating Delta ObjectS\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "if dbutils.fs.ls(\"/Volumes/learning_db/qna/landing/produncts_upsert\"):\n",
    "    \n",
    "    deltaObject = DeltaTable.forPath(spark, \"/Volumes/learning_db/qna/landing/produncts_upsert\")\n",
    "\n",
    "    deltaObject.alias(\"tgt\").merge(\n",
    "        df.alias(\"src\"),\n",
    "        \"src.product_id = tgt.product_id\",\n",
    "    ).whenMatchedUpdateAll(condition=\"src.updated_at >= tgt.updated_at\")\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "\n",
    "else:\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/learning_db/qna/landing/produncts_upsert\")\n",
    "\n",
    "display(spark.sql(\"select * from delta.`/Volumes/learning_db/qna/landing/produncts_upsert`\").orderBy(\"product_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db0311fc-21d2-4f16-9cff-8445485cfc51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final Combined Code: Deduplicate and Upsert Products Table to Delta Lake\n",
    "\n",
    "### Summary of Steps:\n",
    "\n",
    "- **Read** the products table into a Spark DataFrame.\n",
    "- **Deduplicate** by keeping only the latest updated_at for each product_id.\n",
    "- **Upsert** the cleaned DataFrame into the Delta table, merging updates and inserting new records.\n",
    "- **Display** the final upserted Delta table, ordered by product_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5563650-c91d-4657-8d2c-a2491b56a51e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Read the products table into a Spark DataFrame\n",
    "df = spark.table(\"learning_db.qna.products\")\n",
    "\n",
    "# Step 2: Remove duplicate product_id rows, keeping only the latest updated_at for each product\n",
    "from pyspark.sql.functions import col, row_number, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "WindowSpec = Window.partitionBy(\"product_id\").orderBy(desc(\"updated_at\"))\n",
    "df_rnk = df.withColumn(\"rnk\", row_number().over(WindowSpec))\n",
    "df_uqi = df_rnk.filter(col(\"rnk\") == 1).drop(\"rnk\")\n",
    "\n",
    "# Step 3: Upsert the cleaned DataFrame into the Delta table\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "if dbutils.fs.ls(\"/Volumes/learning_db/qna/landing/produncts_upsert\"):\n",
    "    deltaObject = DeltaTable.forPath(spark, \"/Volumes/learning_db/qna/landing/produncts_upsert\")\n",
    "    deltaObject.alias(\"tgt\").merge(\n",
    "        df_uqi.alias(\"src\"),\n",
    "        \"src.product_id = tgt.product_id\",\n",
    "    ).whenMatchedUpdateAll(condition=\"src.updated_at >= tgt.updated_at\")\\\n",
    "     .whenNotMatchedInsertAll()\\\n",
    "     .execute()\n",
    "else:\n",
    "    df_uqi.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/learning_db/qna/landing/produncts_upsert\")\n",
    "\n",
    "# Step 4: Display the upserted Delta table\n",
    "display(spark.sql(\"select * from delta.`/Volumes/learning_db/qna/landing/produncts_upsert`\").orderBy(\"product_id\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5510726138587877,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "retail_catalog_upsert_pyspark_delta",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
